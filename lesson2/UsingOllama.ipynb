{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6e9fa1fc-eac5-4d1d-9be4-541b3f2b3458",
      "metadata": {
        "id": "6e9fa1fc-eac5-4d1d-9be4-541b3f2b3458"
      },
      "source": [
        "# USING OLLAMA\n",
        "\n",
        "---\n",
        "\n",
        "**Benefits:**\n",
        "1. No API charges - open-source\n",
        "2. Data doesn't leave your box\n",
        "\n",
        "**Disadvantages:**\n",
        "1. Significantly less power than Frontier Model\n",
        "\n",
        "## Recap on installation of Ollama\n",
        "\n",
        "Simply visit [ollama.com](https://ollama.com) and install!\n",
        "\n",
        "Once complete, the ollama server should already be running locally.  \n",
        "If you visit:  \n",
        "[http://localhost:11434/](http://localhost:11434/)\n",
        "\n",
        "You should see the message `Ollama is running`.  \n",
        "\n",
        "If not, bring up a new Terminal (Mac) or Powershell (Windows) and enter `ollama serve`  \n",
        "And in another Terminal (Mac) or Powershell (Windows), enter `ollama pull llama3.2`  \n",
        "Then try [http://localhost:11434/](http://localhost:11434/) again.\n",
        "\n",
        "If Ollama is slow on your machine, try using `llama3.2:1b` as an alternative. Run `ollama pull llama3.2:1b` from a Terminal or Powershell, and change the code below from `MODEL = \"llama3.2\"` to `MODEL = \"llama3.2:1b\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e2a9393-7767-488e-a8bf-27c12dca35bd",
      "metadata": {
        "id": "4e2a9393-7767-488e-a8bf-27c12dca35bd"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29ddd15d-a3c5-4f4e-a678-873f56162724",
      "metadata": {
        "id": "29ddd15d-a3c5-4f4e-a678-873f56162724"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "\n",
        "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
        "HEADERS = {\"Content-Type\": \"application/json\"}\n",
        "MODEL = \"llama3.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dac0a679-599c-441f-9bf2-ddc73d35b940",
      "metadata": {
        "id": "dac0a679-599c-441f-9bf2-ddc73d35b940"
      },
      "outputs": [],
      "source": [
        "# Create a messages list using the same format that we used for OpenAI\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Translate This text to Japanese : Thank You\"}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bb9c624-14f0-4945-a719-8ddb64f66f47",
      "metadata": {
        "id": "7bb9c624-14f0-4945-a719-8ddb64f66f47"
      },
      "outputs": [],
      "source": [
        "payload = {\n",
        "        \"model\": MODEL,\n",
        "        \"messages\": messages,\n",
        "        \"stream\": False\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "479ff514-e8bd-4985-a572-2ea28bb4fa40",
      "metadata": {
        "id": "479ff514-e8bd-4985-a572-2ea28bb4fa40",
        "outputId": "4b069a3b-2232-4062-b168-d1f3e2b23f19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
            "pulling dde5aa3fc5ff: 100% ▕██████████████████▏ 2.0 GB                         \u001b[K\n",
            "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
            "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
            "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
            "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
            "pulling 34bb5ab01051: 100% ▕██████████████████▏  561 B                         \u001b[K\n",
            "verifying sha256 digest \u001b[K\n",
            "writing manifest \u001b[K\n",
            "success \u001b[K\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ],
      "source": [
        "# Let's just make sure the model is loaded\n",
        "\n",
        "!ollama pull llama3.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42b9f644-522d-4e05-a691-56e7658c0ea9",
      "metadata": {
        "id": "42b9f644-522d-4e05-a691-56e7658c0ea9",
        "outputId": "47c18efd-2ada-41a5-952b-c07999eb0c44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generative AI has numerous business applications across various industries, including:\n",
            "\n",
            "1. **Content Generation**: AI-powered tools can create high-quality content such as blog posts, articles, product descriptions, and social media posts, saving time and resources for content creators.\n",
            "2. **Personalization**: Generative AI can help personalize customer experiences by generating personalized product recommendations, offers, and marketing messages based on individual customer behavior and preferences.\n",
            "3. **Product Design and Development**: AI-powered generative design tools can create 2D and 3D designs for products, reducing the need for manual design and prototyping.\n",
            "4. **Image and Video Generation**: Generative AI can generate high-quality images and videos that can be used for marketing, advertising, or social media purposes, such as product showcases, explainer videos, or customer testimonials.\n",
            "5. **Chatbots and Virtual Assistants**: AI-powered chatbots can help businesses provide 24/7 customer support, answer frequently asked questions, and route complex queries to human representatives.\n",
            "6. **Predictive Maintenance**: Generative AI can analyze sensor data from machines and predict when maintenance is required, reducing downtime and increasing equipment lifespan.\n",
            "7. **Marketing Automation**: AI-powered marketing automation tools can generate personalized email campaigns, social media posts, and ads based on customer behavior and preferences.\n",
            "8. **Customer Service**: Generative AI can help businesses respond to customer inquiries, resolve simple issues, and provide basic support, freeing up human customer support agents for more complex cases.\n",
            "9. **Data Analysis and Visualization**: AI-powered tools can analyze large datasets, identify patterns, and generate visualizations that help businesses make data-driven decisions.\n",
            "10. **Creative Writing and Storytelling**: Generative AI can create original stories, articles, and content in various formats, such as scripts, poetry, or dialogue.\n",
            "\n",
            "Some specific business applications of Generative AI include:\n",
            "\n",
            "* **Amazon's Product Recommendations**: Amazon uses generative AI to personalize product recommendations for customers based on their browsing and purchasing history.\n",
            "* **Google's Image Search**: Google's image search algorithm uses generative AI to improve the accuracy and relevance of search results.\n",
            "* **DHL's Supply Chain Optimization**: DHL uses generative AI to optimize its supply chain management, predicting demand and reducing inventory levels.\n",
            "* **H&M's Fashion Design**: H&M uses generative AI to create new fashion designs and patterns for their clothing lines.\n",
            "* **Lyft's Self-Driving Cars**: Lyft is using generative AI to develop autonomous vehicles that can navigate roads safely.\n",
            "\n",
            "These are just a few examples of the many business applications of Generative AI. As the technology continues to evolve, we can expect to see even more innovative uses across various industries.\n"
          ]
        }
      ],
      "source": [
        "# If this doesn't work for any reason, try the 2 versions in the following cells\n",
        "# And double check the instructions in the 'Recap on installation of Ollama' at the top of this lab\n",
        "# And if none of that works - contact me!\n",
        "\n",
        "response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
        "print(response.json()['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a021f13-d6a1-4b96-8e18-4eae49d876fe",
      "metadata": {
        "id": "6a021f13-d6a1-4b96-8e18-4eae49d876fe"
      },
      "source": [
        "# Introducing the ollama package\n",
        "\n",
        "And now we'll do the same thing, but using the elegant ollama python package instead of a direct HTTP call.\n",
        "\n",
        "Under the hood, it's making the same call as above to the ollama server running at localhost:11434"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7745b9c4-57dc-4867-9180-61fa5db55eb8",
      "metadata": {
        "id": "7745b9c4-57dc-4867-9180-61fa5db55eb8",
        "outputId": "5eddaa05-afd2-4575-9a35-2ca22d15583e"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'ollama'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mollama\u001b[39;00m\n\u001b[32m      3\u001b[39m response = ollama.chat(model=MODEL, messages=messages)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(response[\u001b[33m'\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m'\u001b[39m])\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'ollama'"
          ]
        }
      ],
      "source": [
        "import ollama\n",
        "\n",
        "response = ollama.chat(model=MODEL, messages=messages)\n",
        "print(response['message']['content'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4704e10-f5fb-4c15-a935-f046c06fb13d",
      "metadata": {
        "id": "a4704e10-f5fb-4c15-a935-f046c06fb13d"
      },
      "source": [
        "## Alternative approach - using OpenAI python library to connect to Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23057e00-b6fc-4678-93a9-6b31cb704bff",
      "metadata": {
        "id": "23057e00-b6fc-4678-93a9-6b31cb704bff",
        "outputId": "1fa75247-f226-4053-a2b0-6fa8feab16df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generative AI has numerous business applications across various industries, including:\n",
            "\n",
            "1. **Content Creation**: Generate high-quality content such as articles, social media posts, product descriptions, and even entire books. Examples include content mills, news agencies, and marketing firms.\n",
            "2. **Graphic Design and Visual Content**: Create visual content like logos, images, videos, and animations using generative AI algorithms. This is useful for graphic design agencies, ad agencies, and social media platforms.\n",
            "3. **Music Composition and Soundscapes**: Generate music, sound effects, or even entire tracks using generative music models. Examples include music production studios, gaming companies, and film productions.\n",
            "4. **Product Design and Prototyping**: Use generative AI to create 2D and 3D product designs, prototypes, and even entire product lines. This is useful for fashion designers, product developers, and industrial design firms.\n",
            "5. **Marketing and Advertising**: Generate personalized ad content, social media ads, and marketing campaigns using generative AI. Examples include digital marketing agencies, ad tech companies, and e-commerce platforms.\n",
            "6. **Translation and Localization**: Develop language models that can translate text and speech in real-time, enabling businesses to tap into international markets more efficiently. Examples include translation software, interpretation services, and language learning apps.\n",
            "7. **Chatbots and Virtual Assistants**: Create conversational interfaces for chatbots, virtual assistants, and customer service platforms using generative AI. Examples include customer support software, conversational UX design firms, and voice assistant developers.\n",
            "8. **Predictive Analytics and Forecasts**: Use generative AI to predict market trends, forecast sales, and make informed business decisions. Examples include data analytics firms, financial institutions, and market research companies.\n",
            "9. **Generative Text Analysis and Interpretation**: Develop models that can analyze and interpret large amounts of text data, such as sentiment analysis, entity recognition, and topic modeling. Examples include natural language processing (NLP) researchers, social media monitoring tools, and e-discovery firms.\n",
            "10. **Digital Twinning and Simulation**: Create virtual replicas of physical processes and systems using generative AI algorithms. This enables businesses to simulate and analyze complex systems more efficiently. Examples include industrial simulation software, energy management platforms, and architectural design companies.\n",
            "\n",
            "Additionally, generative AI has the potential to transform various industries by:\n",
            "\n",
            "* Automating routine tasks and freeing up human resources for higher-value work\n",
            "* Enabling new forms of content creation and storytelling\n",
            "* Enhancing customer experiences through personalized interactions and services\n",
            "* Informing business decisions with more accurate predictive analytics and forecasting\n",
            "\n",
            "However, it's also essential to acknowledge the challenges and limitations associated with generative AI in various industries, such as:\n",
            "\n",
            "* Ensuring fairness, accuracy, and transparency in generated content\n",
            "* Navigating intellectual property rights and authorship in digital creations\n",
            "* Addressing biases and potential misuses of AI-generated content\n",
            "* Developing robust business models and value propositions around generative AI applications\n"
          ]
        }
      ],
      "source": [
        "# There's actually an alternative approach that some people might prefer\n",
        "# You can use the OpenAI client python library to call Ollama:\n",
        "\n",
        "from openai import OpenAI\n",
        "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
        "\n",
        "response = ollama_via_openai.chat.completions.create(\n",
        "    model=MODEL,\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f9e22da-b891-41f6-9ac9-bd0c0a5f4f44",
      "metadata": {
        "id": "9f9e22da-b891-41f6-9ac9-bd0c0a5f4f44"
      },
      "source": [
        "## Are you confused about why that works?\n",
        "\n",
        "It seems strange, right? We just used OpenAI code to call Ollama?? What's going on?!\n",
        "\n",
        "Here's the scoop:\n",
        "\n",
        "The python class `OpenAI` is simply code written by OpenAI engineers that makes calls over the internet to an endpoint.  \n",
        "\n",
        "When you call `openai.chat.completions.create()`, this python code just makes a web request to the following url: \"https://api.openai.com/v1/chat/completions\"\n",
        "\n",
        "Code like this is known as a \"client library\" - it's just wrapper code that runs on your machine to make web requests. The actual power of GPT is running on OpenAI's cloud behind this API, not on your computer!\n",
        "\n",
        "OpenAI was so popular, that lots of other AI providers provided identical web endpoints, so you could use the same approach.\n",
        "\n",
        "So Ollama has an endpoint running on your local box at http://localhost:11434/v1/chat/completions  \n",
        "And in week 2 we'll discover that lots of other providers do this too, including Gemini and DeepSeek.\n",
        "\n",
        "And then the team at OpenAI had a great idea: they can extend their client library so you can specify a different 'base url', and use their library to call any compatible API.\n",
        "\n",
        "That's it!\n",
        "\n",
        "So when you say: `ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')`  \n",
        "Then this will make the same endpoint calls, but to Ollama instead of OpenAI."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc7d1de3-e2ac-46ff-a302-3b4ba38c4c90",
      "metadata": {
        "id": "bc7d1de3-e2ac-46ff-a302-3b4ba38c4c90"
      },
      "source": [
        "## Also trying the amazing reasoning model DeepSeek\n",
        "\n",
        "Here we use the version of DeepSeek-reasoner that's been distilled to 1.5B.  \n",
        "This is actually a 1.5B variant of Qwen that has been fine-tuned using synethic data generated by Deepseek R1.\n",
        "\n",
        "Other sizes of DeepSeek are [here](https://ollama.com/library/deepseek-r1) all the way up to the full 671B parameter version, which would use up 404GB of your drive and is far too large for most!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf9eb44e-fe5b-47aa-b719-0bb63669ab3d",
      "metadata": {
        "id": "cf9eb44e-fe5b-47aa-b719-0bb63669ab3d"
      },
      "outputs": [],
      "source": [
        "!ollama pull deepseek-r1:1.5b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d3d554b-e00d-4c08-9300-45e073950a76",
      "metadata": {
        "id": "1d3d554b-e00d-4c08-9300-45e073950a76",
        "outputId": "7c33db7a-62d8-4b08-dcb1-769c3f143596"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<think>\n",
            "Okay, I need to explain the key concepts behind Large Language Models (LLMs), specifically focusing on what neural networks are, attention mechanisms in transformers, and maybe something about how transformers themselves work. Let me break this down step by step.\n",
            "\n",
            "First, understanding what a neural network is. Neural networks are computational models inspired by the structure of the human brain. They consist of layers, nodes (or neurons) connected together with weights that represent connections or dependencies. So these models process information through layers where each layer transforms the inputs into representations that can be used for the next layer. Think of it as a way to model complex patterns in data.\n",
            "\n",
            "Next, attention mechanisms in transformers. I remember transformers are known for their efficiency and effectiveness in NLP tasks. The key part here is how they manage the flow of information. The mechanism called \" attended self-attention\" allows each token in a sequence (like words or genes) to attend to a subset of all tokens that come after it. This creates local context, helping the model understand which parts of the input are most relevant when making decisions.\n",
            "\n",
            "Then, what exactly is a transformer? Transformers are a type of neural network architecture designed by researchers like Vaswani and others. Unlike convolutional architectures that focus on spatial features, transformers process data in permutation-invariant vector spaces, meaning their representations don't depend on the order of input tokens. Each token attends to previous and future tokens, which is a unique feature compared to most other models.\n",
            "\n",
            "Putting it all together: LLMs leverage neural networks with attention mechanisms, particularly the transformer's ability to learn local context through self-attention. The architecture allows each model or \"agent\" in an NLP task to attend only locally to its neighbors and past experiences, enabling them to adapt and solve complex problems based on contextual information.\n",
            "\n",
            "I think that covers all the main points without getting too detailed. Maybe add a bit of summary at the end for clarity.\n",
            "</think>\n",
            "\n",
            "Large Language Models (LLMs) are sophisticated AI systems designed to process and generate human-level text or speech. At their core, they employ neural networks with attention mechanisms:\n",
            "\n",
            "1. **Neural Networks**: These are computational models composed of layers of nodes, interconnected through weighted connections. Each layer transforms input into representations that can be used for the next layer, allowing them to model complex patterns in data.\n",
            "\n",
            "2. **Attention Mechanisms**: Specifically, self-attention within transformer architectures enable models to focus on local context. Each token attends only to previous and future tokens, creating hierarchical representations that capture local relationships.\n",
            "\n",
            "3. **Transformer Architecture**: Unlike traditional neural networks with fixed layers, transformers use a permutation-invariant vector space. Each token attends globally but locally, processing input through self-attention, enabling them to learn context and adapt.\n",
            "\n",
            "In summary, LLMs leverage neural networks with attention mechanisms, particularly the transformer's ability to attend only local information, allowing them to process data effectively for tasks like text generation and translation.\n"
          ]
        }
      ],
      "source": [
        "# This may take a few minutes to run! You should then see a fascinating \"thinking\" trace inside <think> tags, followed by some decent definitions\n",
        "\n",
        "response = ollama_via_openai.chat.completions.create(\n",
        "    model=\"deepseek-r1:1.5b\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Please give definitions of some core concepts behind LLMs: a neural network, attention and the transformer\"}]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}